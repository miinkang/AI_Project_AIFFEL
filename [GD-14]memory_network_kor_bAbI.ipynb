{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "closing-transparency",
   "metadata": {},
   "source": [
    "# [Going Deeper NLP 14] 한국어 QA 모델 만들기\n",
    "한국어 bAbI 데이터셋으로 memory network를 구현해 학습한다. \n",
    "\n",
    "---\n",
    "\n",
    "## 프로젝트 목표\n",
    "---\n",
    "- customized konlpy를 사용한다.\n",
    "- memory network를 이해하고 구현한다.  \n",
    "\n",
    "## 프로젝트 설명\n",
    "---\n",
    "1. 단어 토큰화\n",
    "    - customized konlpy를 사용해 사전을 만든다.\n",
    "    \n",
    "2. 불용어 처리 \n",
    "3. memory network를 한국어 데이터로 성능 비교하기 \n",
    "\n",
    "    \n",
    "## 0. Import module, library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alone-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import tarfile\n",
    "from nltk import FreqDist\n",
    "from functools import reduce\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "motivated-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/babi_memory_net'\n",
    "train_path = data_dir + '/qa1_single-supporting-fact_train_kor.txt'\n",
    "test_path = data_dir + '/qa1_single-supporting-fact_test_kor.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-israel",
   "metadata": {},
   "source": [
    "- 데이터 샘플 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "proper-battlefield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 필웅이는 화장실로 갔습니다.\n",
      "2 은경이는 복도로 이동했습니다.\n",
      "3 필웅이는 어디야? \t화장실\t1\n",
      "4 수종이는 복도로 복귀했습니다.\n",
      "5 경임이는 정원으로 갔습니다.\n",
      "6 수종이는 어디야? \t복도\t4\n",
      "7 은경이는 사무실로 갔습니다.\n",
      "8 경임이는 화장실로 뛰어갔습니다.\n",
      "9 수종이는 어디야? \t복도\t4\n",
      "10 필웅이는 복도로 갔습니다.\n",
      "11 수종이는 사무실로 가버렸습니다.\n",
      "12 수종이는 어디야? \t사무실\t11\n",
      "13 은경이는 정원으로 복귀했습니다.\n",
      "14 은경이는 침실로 갔습니다.\n",
      "15 경임이는 어디야? \t화장실\t8\n",
      "1 경임이는 사무실로 가버렸습니다.\n",
      "2 경임이는 화장실로 이동했습니다.\n",
      "3 경임이는 어디야? \t화장실\t2\n",
      "4 필웅이는 침실로 이동했습니다.\n",
      "5 수종이는 복도로 갔습니다.\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "lines = open(train_path , \"rb\")\n",
    "for line in lines:\n",
    "    line = line.decode(\"utf-8\").strip()\n",
    "    # lno, text = line.split(\" \", 1) # ID와 TEXT 분리\n",
    "    i = i + 1\n",
    "    print(line)\n",
    "    if i == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-motor",
   "metadata": {},
   "source": [
    "## bAbI 데이터 설명 \n",
    "- 기계 이해 모델 중 하나인 **Memory Network** task에 적합한 데이터셋 \n",
    "- 모델이 텍스트 내용에 대한 추론을 할 수 있도록 구성된 데이터셋\n",
    "---\n",
    "- 정보 문장이 연속해서 나온다. \n",
    "- 질문과 답으로 구성되어있는 문장이 나온다. \n",
    "    - 질문, 정답, 정답의 근거가 되는 문장의 index\n",
    "- 새로운 스토리(정보 문장)이 시작되면 맨 앞 id number는 다시 1로 초기화된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-young",
   "metadata": {},
   "source": [
    "## 1. 데이터 전처리 \n",
    "### 기본 전처리\n",
    "- **스토리, 질문, 답변**으로 분리해 저장하는 과정 \n",
    "- supporting fact(정답의 근거가 되는 문장의 Index)는 저장하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "numeric-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(dir):\n",
    "    stories, questions, answers = [], [], []\n",
    "    story_temp = []  # 현재 시점의 스토리 \n",
    "    lines = open(dir, 'rb')\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8')\n",
    "        line = line.strip() # '\\n' 제거\n",
    "        idx, text = line.split(' ', 1)  # split(separator, maxsplit) : 1번 문자열 나눠 저장\n",
    "        \n",
    "        if int(idx) == 1: # 스토리별 분리 \n",
    "            story_temp = [] # 초기화\n",
    "        \n",
    "        if '\\t' in text: # 해당 줄이 질문과 답변이 있는 경우\n",
    "            question, answer, _ = text.split('\\t')\n",
    "            stories.append([x for x in story_temp if x]) # 누적 스토리 저장 \n",
    "            questions.append(question)\n",
    "            answers.append(answer)\n",
    "        \n",
    "        else: \n",
    "            story_temp.append(text)\n",
    "    \n",
    "    lines.close()\n",
    "    return stories, questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "capital-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stories, train_questions, train_answers = read_data(train_path)\n",
    "test_stories, test_questions, test_answers = read_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "increased-following",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 스토리 개수: 10000\n",
      "train 질문 개수: 10000\n",
      "train 답변 개수: 10000\n",
      "test 스토리 개수: 1000\n",
      "test 질문 개수: 1000\n",
      "test 답변 개수: 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"train 스토리 개수:\", len(train_stories))\n",
    "print(\"train 질문 개수:\", len(train_questions))\n",
    "print(\"train 답변 개수:\", len(train_answers))\n",
    "print(\"test 스토리 개수:\", len(test_stories))\n",
    "print(\"test 질문 개수:\", len(test_questions))\n",
    "print(\"test 답변 개수:\", len(test_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "changing-volunteer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['필웅이는 화장실로 갔습니다.', '은경이는 복도로 이동했습니다.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stories[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "domestic-compound",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['필웅이는 어디야? ', '수종이는 어디야? ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_questions[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "behavioral-divorce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['화장실', '복도']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_answers[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-reception",
   "metadata": {},
   "source": [
    "## 2. 데이터 토큰화\n",
    "- customized konlpy를 사용해 토큰화한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "derived-motivation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckonlpy.tag import Twitter\n",
    "\n",
    "twitter = Twitter()\n",
    "twitter.add_dictionary('은경이', 'Noun')\n",
    "test_morph = twitter.morphs('은경이는 사무실로 갔습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "permanent-challenge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['은경이', '는', '사무실', '로', '갔습니다', '.'], list)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_morph, type(test_morph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "preliminary-exception",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token(sentence):\n",
    "    twitter = Twitter()\n",
    "    return twitter.morphs(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-america",
   "metadata": {},
   "source": [
    "- 함수 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "regional-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stories_token_test = [get_token(sen) for sen in train_stories[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fleet-reference",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['필웅이', '는', '화장실', '로', '갔습니다', '.'],\n",
       " ['은', '경이', '는', '복도', '로', '이동', '했습니다', '.']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stories_token_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-andorra",
   "metadata": {},
   "source": [
    "- 토큰 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "responsible-leadership",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-6671d4c92f7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_stories_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msen\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_stories\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# train_questions_token = [get_token(sen) for sen in train_questions]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# train_answers_token = [get_token(sen) for sen in train_answers]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# test_stories_token = [get_token(sen) for sen in test_stories]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# test_questions_token = [get_token(sen) for sen in test_questions]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-6671d4c92f7b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_stories_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msen\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_stories\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# train_questions_token = [get_token(sen) for sen in train_questions]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# train_answers_token = [get_token(sen) for sen in train_answers]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# test_stories_token = [get_token(sen) for sen in test_stories]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# test_questions_token = [get_token(sen) for sen in test_questions]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-e6182334c43f>\u001b[0m in \u001b[0;36mget_token\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtwitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtwitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmorphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/ckonlpy/tag/_abstract.py\u001b[0m in \u001b[0;36mmorphs\u001b[0;34m(self, phrase, norm, stem)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmorphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mphrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/ckonlpy/tag/_abstract.py\u001b[0m in \u001b[0;36mpos\u001b[0;34m(self, phrase, norm, stem, perfect_match)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwordpos_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0meojeols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphrase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mtagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "train_stories_token = [get_token(sen) for sen in train_stories]\n",
    "# train_questions_token = [get_token(sen) for sen in train_questions]\n",
    "# train_answers_token = [get_token(sen) for sen in train_answers]\n",
    "# test_stories_token = [get_token(sen) for sen in test_stories]\n",
    "# test_questions_token = [get_token(sen) for sen in test_questions]\n",
    "# test_answers_token = [get_token(sen) for sen in test_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-annotation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-landing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-medicaid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "operating-fantasy",
   "metadata": {},
   "source": [
    "# 루브릭 평가\n",
    "---\n",
    "1. **한국어의 특성에 알맞게 전처리가 진행되었다.**   \n",
    "\n",
    "    - 한국어 특성에 따른 토큰화, 임베딩을 거쳐 데이터셋이 적절히 구성되었다.\n",
    "\n",
    "\n",
    "2. **메모리 네트워크가 정상적으로 구현되어 학습이 안정적으로 진행되었다.**\n",
    "\n",
    "    - validation loss가 안정적으로 수렴하는 것을 확인하고 이를 시각화하였다.\n",
    "\n",
    "\n",
    "3. **메모리 네트워크를 통해 한국어 bAbI 태스크의 높은 정확도를 달성하였다.** \n",
    " \n",
    "     - 추론 태스크의 테스트 정확도가 90% 이상 달성하였다.\n",
    "\n",
    "\n",
    "    \n",
    "# 회고\n",
    "---\n",
    "## 어려웠던 부분 \n",
    "- \n",
    "\n",
    "## 알아낸 점 혹은 모호한 부분 \n",
    "- \n",
    "\n",
    "## 느낀 점 \n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
