{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "local-assumption",
   "metadata": {},
   "source": [
    "# [Exploration 06] 멋진 작사가 만들기\n",
    "## 01. download data\n",
    "- 학습할 텍스트 데이터를 다운로드합니다.\n",
    "- 해당 프로젝트에서는 작사가를 만들기 위해 가사 데이터를 다운로드합니다. \n",
    "\n",
    "## 02. load data \n",
    "- glob 모듈을 사용해 파일을 읽습니다. \n",
    "- 문장 단위로 끊어서 corpus(말뭉치 데이터)를 저장합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fabulous-french",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['I. LIFE.', '', '']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    # with구문으로 txt_list에 있는 txt_file 하나하나를 열어줍니다.\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        # line을 기준으로 문장단위로 끊어서 raw변수에 넣어줍니다.\n",
    "        raw = f.read().splitlines()\n",
    "        # 리스트로 생성된 raw는 append가 아닌 extend로 raw_corpus에 더합니다.\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-twins",
   "metadata": {},
   "source": [
    "## 03. preprocess data \n",
    "- 적절한 문장의 길이를 구해서, padding을 하거나 잘라줍니다.\n",
    "- tf.keras.preprocessing.text.Tokenizer 패키지를 이용해 정제된 데이터를 토큰화합니다.\n",
    "- 토큰을 모아 사전을 만들고, 숫자로 변환해줍니다.\n",
    "- 이때 숫자로 변환된 데이터(해당 프로젝트에서는 노래 가사)를 텐서(tensor)라고 합니다.\n",
    "- 해당 프로젝트에서는 한 문장에 토큰 15개를 권장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "harmful-jacksonville",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I. LIFE.',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'I.',\n",
       " '',\n",
       " 'SUCCESS.',\n",
       " '',\n",
       " '[Published in \"A Masque of Poets\"',\n",
       " 'at the request of \"H.H.,\" the author\\'s',\n",
       " 'fellow-townswoman and friend.]',\n",
       " '',\n",
       " 'Success is counted sweetest']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_corpus[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "proprietary-serve",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()      \n",
    "  \n",
    "    # 불필요한 특수기호, 공백 등의 패턴을 지우고 공백으로 단어를 토큰화할 수 있도록 정제합니다\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \", sentence)        # 패턴의 특수문자 -> 공백\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴 -> 스페이스 1개\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도) -> 스페이스 1개\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "outer-concentration",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> published in a masque of poets <end>'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = '[Published in \"A Masque of Poets\"'\n",
    "sentence_1 = preprocess_sentence(sentence)\n",
    "sentence_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "adjusted-insert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> i life <end>',\n",
       " '<start> i <end>',\n",
       " '<start> success <end>',\n",
       " '<start> published in a masque of poets <end>',\n",
       " '<start> at the request of h h the author s <end>',\n",
       " '<start> fellow townswoman and friend <end>',\n",
       " '<start> success is counted sweetest <end>',\n",
       " '<start> by those who ne er succeed <end>',\n",
       " '<start> to comprehend a nectar <end>',\n",
       " '<start> requires sorest need <end>']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0:\n",
    "        continue\n",
    "#     if len(sentence) > 15:\n",
    "#         sentence = sentence[:15]\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-photography",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "comic-nutrition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    4  156 ...    0    0    0]\n",
      " [   2    4    3 ...    0    0    0]\n",
      " [   2 3098    3 ...    0    0    0]\n",
      " ...\n",
      " [   2    5  832 ...    0    0    0]\n",
      " [   2   21   54 ...    0    0    0]\n",
      " [   2   51    4 ...    0    0    0]] \n",
      " <keras_preprocessing.text.Tokenizer object at 0x7f4228789110>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(175986, 175986)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def tokenize(corpus):\n",
    "    # Tokenizer 패키지를 생성합니다.\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,  # 단어장의 크기를 설정합니다. (권장 12,000 이상)\n",
    "        filters=' ',    # 별도의 전처리 로직\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen는 시퀀스 길이를 뜻합니다. 지정하지 않을 때는 None이 디폴트값입니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "\n",
    "    print(tensor,'\\n', tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n",
    "\n",
    "len(tensor), len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-addition",
   "metadata": {},
   "source": [
    "## 04. split data\n",
    "## 05. build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-burns",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-mailman",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-harbor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-large",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-trace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-mirror",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-soviet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-activity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
